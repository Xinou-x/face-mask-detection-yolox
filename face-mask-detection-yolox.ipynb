{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":1176415,"sourceType":"datasetVersion","datasetId":667889}],"dockerImageVersionId":30301,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!git clone https://github.com/bubbliiiing/yolox-pytorch.git","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-11-01T06:20:42.733614Z","iopub.execute_input":"2022-11-01T06:20:42.734048Z","iopub.status.idle":"2022-11-01T06:20:46.472843Z","shell.execute_reply.started":"2022-11-01T06:20:42.733954Z","shell.execute_reply":"2022-11-01T06:20:46.471499Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%cd /kaggle/working/yolox-pytorch","metadata":{"execution":{"iopub.status.busy":"2022-11-01T06:20:46.475323Z","iopub.execute_input":"2022-11-01T06:20:46.476088Z","iopub.status.idle":"2022-11-01T06:20:46.484509Z","shell.execute_reply.started":"2022-11-01T06:20:46.476037Z","shell.execute_reply":"2022-11-01T06:20:46.483297Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!rm -rf VOCdevkit/VOC2007/Annotations/README.md\n!rm -rf VOCdevkit/VOC2007/JPEGImages/README.md","metadata":{"execution":{"iopub.status.busy":"2022-11-01T06:20:46.48744Z","iopub.execute_input":"2022-11-01T06:20:46.488848Z","iopub.status.idle":"2022-11-01T06:20:48.436066Z","shell.execute_reply.started":"2022-11-01T06:20:46.488535Z","shell.execute_reply":"2022-11-01T06:20:48.434652Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nfrom shutil import copy\nfrom sys import exit\nimport tqdm\nimport os\nimport cv2\nimport numpy as np\nfrom PIL import Image\npath = \"/kaggle/input/face-mask-detection/images\" #文件夹目录\npath_map = \"/kaggle/input/face-mask-detection/annotations\"\nfiles= os.listdir(path) #得到文件夹下的所有文件名称\n\nfor i in range(len(files)):\n    if os.path.splitext(files[i])[1] == '.png' or '.PNG':\n        name = os.path.splitext(files[i])[0]\n        newFileName = name + \".jpg\"\n    im = Image.open(path+'/'+files[i])\n    im_gray1 = np.array(im)\n    cv2.imwrite('/kaggle/working/yolox-pytorch/VOCdevkit/VOC2007/JPEGImages/' + newFileName, im_gray1)\n\n    #copy(path+'/'+files[i], '/kaggle/working/yolox-pytorch/VOCdevkit/VOC2007/JPEGImages')\n    copy(path_map+'/'+files[i][:-4]+'.xml', '/kaggle/working/yolox-pytorch/VOCdevkit/VOC2007/Annotations')\n    ","metadata":{"execution":{"iopub.status.busy":"2022-11-01T06:20:48.440347Z","iopub.execute_input":"2022-11-01T06:20:48.440694Z","iopub.status.idle":"2022-11-01T06:21:15.709616Z","shell.execute_reply.started":"2022-11-01T06:20:48.440658Z","shell.execute_reply":"2022-11-01T06:21:15.708653Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%writefile model_data/voc_classes.txt\nwith_mask\nwithout_mask\nmask_weared_incorrect","metadata":{"execution":{"iopub.status.busy":"2022-11-01T06:21:15.710958Z","iopub.execute_input":"2022-11-01T06:21:15.713586Z","iopub.status.idle":"2022-11-01T06:21:15.720734Z","shell.execute_reply.started":"2022-11-01T06:21:15.713555Z","shell.execute_reply":"2022-11-01T06:21:15.719671Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!python voc_annotation.py","metadata":{"execution":{"iopub.status.busy":"2022-11-01T06:21:15.721983Z","iopub.execute_input":"2022-11-01T06:21:15.722958Z","iopub.status.idle":"2022-11-01T06:21:17.092797Z","shell.execute_reply.started":"2022-11-01T06:21:15.722916Z","shell.execute_reply":"2022-11-01T06:21:17.091643Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%cd model_data\n!wget https://github.com/bubbliiiing/yolox-pytorch/releases/download/v1.0/yolox_s.pth","metadata":{"execution":{"iopub.status.busy":"2022-11-01T06:21:18.042247Z","iopub.execute_input":"2022-11-01T06:21:18.043879Z","iopub.status.idle":"2022-11-01T06:21:27.927477Z","shell.execute_reply.started":"2022-11-01T06:21:18.043833Z","shell.execute_reply":"2022-11-01T06:21:27.9263Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%cd /kaggle/working/yolox-pytorch/","metadata":{"execution":{"iopub.status.busy":"2022-11-01T06:21:27.929623Z","iopub.execute_input":"2022-11-01T06:21:27.930035Z","iopub.status.idle":"2022-11-01T06:21:27.938592Z","shell.execute_reply.started":"2022-11-01T06:21:27.929993Z","shell.execute_reply":"2022-11-01T06:21:27.937475Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%writefile train.py\n#-------------------------------------#\n#       对数据集进行训练\n#-------------------------------------#\nimport datetime\nimport os\n\nimport numpy as np\nimport torch\nimport torch.backends.cudnn as cudnn\nimport torch.distributed as dist\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\n\nfrom nets.yolo import YoloBody\nfrom nets.yolo_training import (ModelEMA, YOLOLoss, get_lr_scheduler,\n                                set_optimizer_lr, weights_init)\nfrom utils.callbacks import LossHistory, EvalCallback\nfrom utils.dataloader import YoloDataset, yolo_dataset_collate\nfrom utils.utils import get_classes, show_config\nfrom utils.utils_fit import fit_one_epoch\n\n'''\n训练自己的目标检测模型一定需要注意以下几点：\n1、训练前仔细检查自己的格式是否满足要求，该库要求数据集格式为VOC格式，需要准备好的内容有输入图片和标签\n   输入图片为.jpg图片，无需固定大小，传入训练前会自动进行resize。\n   灰度图会自动转成RGB图片进行训练，无需自己修改。\n   输入图片如果后缀非jpg，需要自己批量转成jpg后再开始训练。\n\n   标签为.xml格式，文件中会有需要检测的目标信息，标签文件和输入图片文件相对应。\n\n2、损失值的大小用于判断是否收敛，比较重要的是有收敛的趋势，即验证集损失不断下降，如果验证集损失基本上不改变的话，模型基本上就收敛了。\n   损失值的具体大小并没有什么意义，大和小只在于损失的计算方式，并不是接近于0才好。如果想要让损失好看点，可以直接到对应的损失函数里面除上10000。\n   训练过程中的损失值会保存在logs文件夹下的loss_%Y_%m_%d_%H_%M_%S文件夹中\n   \n3、训练好的权值文件保存在logs文件夹中，每个训练世代（Epoch）包含若干训练步长（Step），每个训练步长（Step）进行一次梯度下降。\n   如果只是训练了几个Step是不会保存的，Epoch和Step的概念要捋清楚一下。\n'''\nif __name__ == \"__main__\":\n    #---------------------------------#\n    #   Cuda    是否使用Cuda\n    #           没有GPU可以设置成False\n    #---------------------------------#\n    Cuda            = True\n    #---------------------------------------------------------------------#\n    #   distributed     用于指定是否使用单机多卡分布式运行\n    #                   终端指令仅支持Ubuntu。CUDA_VISIBLE_DEVICES用于在Ubuntu下指定显卡。\n    #                   Windows系统下默认使用DP模式调用所有显卡，不支持DDP。\n    #   DP模式：\n    #       设置            distributed = False\n    #       在终端中输入    CUDA_VISIBLE_DEVICES=0,1 python train.py\n    #   DDP模式：\n    #       设置            distributed = True\n    #       在终端中输入    CUDA_VISIBLE_DEVICES=0,1 python -m torch.distributed.launch --nproc_per_node=2 train.py\n    #---------------------------------------------------------------------#\n    distributed     = False\n    #---------------------------------------------------------------------#\n    #   sync_bn     是否使用sync_bn，DDP模式多卡可用\n    #---------------------------------------------------------------------#\n    sync_bn         = False\n    #---------------------------------------------------------------------#\n    #   fp16        是否使用混合精度训练\n    #               可减少约一半的显存、需要pytorch1.7.1以上\n    #---------------------------------------------------------------------#\n    fp16            = False\n    #---------------------------------------------------------------------#\n    #   classes_path    指向model_data下的txt，与自己训练的数据集相关 \n    #                   训练前一定要修改classes_path，使其对应自己的数据集\n    #---------------------------------------------------------------------#\n    classes_path    = 'model_data/voc_classes.txt'\n    #----------------------------------------------------------------------------------------------------------------------------#\n    #   权值文件的下载请看README，可以通过网盘下载。模型的 预训练权重 对不同数据集是通用的，因为特征是通用的。\n    #   模型的 预训练权重 比较重要的部分是 主干特征提取网络的权值部分，用于进行特征提取。\n    #   预训练权重对于99%的情况都必须要用，不用的话主干部分的权值太过随机，特征提取效果不明显，网络训练的结果也不会好\n    #\n    #   如果训练过程中存在中断训练的操作，可以将model_path设置成logs文件夹下的权值文件，将已经训练了一部分的权值再次载入。\n    #   同时修改下方的 冻结阶段 或者 解冻阶段 的参数，来保证模型epoch的连续性。\n    #   \n    #   当model_path = ''的时候不加载整个模型的权值。\n    #\n    #   此处使用的是整个模型的权重，因此是在train.py进行加载的。\n    #   如果想要让模型从0开始训练，则设置model_path = ''，下面的Freeze_Train = Fasle，此时从0开始训练，且没有冻结主干的过程。\n    #   \n    #   一般来讲，网络从0开始的训练效果会很差，因为权值太过随机，特征提取效果不明显，因此非常、非常、非常不建议大家从0开始训练！\n    #   从0开始训练有两个方案：\n    #   1、得益于Mosaic数据增强方法强大的数据增强能力，将UnFreeze_Epoch设置的较大（300及以上）、batch较大（16及以上）、数据较多（万以上）的情况下，\n    #      可以设置mosaic=True，直接随机初始化参数开始训练，但得到的效果仍然不如有预训练的情况。（像COCO这样的大数据集可以这样做）\n    #   2、了解imagenet数据集，首先训练分类模型，获得网络的主干部分权值，分类模型的 主干部分 和该模型通用，基于此进行训练。\n    #----------------------------------------------------------------------------------------------------------------------------#\n    model_path      = 'model_data/yolox_s.pth'\n    #------------------------------------------------------#\n    #   input_shape     输入的shape大小，一定要是32的倍数\n    #------------------------------------------------------#\n    input_shape     = [640, 640]\n    #------------------------------------------------------#\n    #   所使用的YoloX的版本。nano、tiny、s、m、l、x\n    #------------------------------------------------------#\n    phi             = 's'\n    #------------------------------------------------------------------#\n    #   mosaic              马赛克数据增强。\n    #   mosaic_prob         每个step有多少概率使用mosaic数据增强，默认50%。\n    #\n    #   mixup               是否使用mixup数据增强，仅在mosaic=True时有效。\n    #                       只会对mosaic增强后的图片进行mixup的处理。\n    #   mixup_prob          有多少概率在mosaic后使用mixup数据增强，默认50%。\n    #                       总的mixup概率为mosaic_prob * mixup_prob。\n    #\n    #   special_aug_ratio   参考YoloX，由于Mosaic生成的训练图片，远远脱离自然图片的真实分布。\n    #                       当mosaic=True时，本代码会在special_aug_ratio范围内开启mosaic。\n    #                       默认为前70%个epoch，100个世代会开启70个世代。\n    #\n    #   余弦退火算法的参数放到下面的lr_decay_type中设置\n    #------------------------------------------------------------------#\n    mosaic              = True\n    mosaic_prob         = 0.5\n    mixup               = True\n    mixup_prob          = 0.5\n    special_aug_ratio   = 0.7\n\n    #----------------------------------------------------------------------------------------------------------------------------#\n    #   训练分为两个阶段，分别是冻结阶段和解冻阶段。设置冻结阶段是为了满足机器性能不足的同学的训练需求。\n    #   冻结训练需要的显存较小，显卡非常差的情况下，可设置Freeze_Epoch等于UnFreeze_Epoch，Freeze_Train = True，此时仅仅进行冻结训练。\n    #      \n    #   在此提供若干参数设置建议，各位训练者根据自己的需求进行灵活调整：\n    #   （一）从整个模型的预训练权重开始训练： \n    #       Adam：\n    #           Init_Epoch = 0，Freeze_Epoch = 50，UnFreeze_Epoch = 100，Freeze_Train = True，optimizer_type = 'adam'，Init_lr = 1e-3，weight_decay = 0。（冻结）\n    #           Init_Epoch = 0，UnFreeze_Epoch = 100，Freeze_Train = False，optimizer_type = 'adam'，Init_lr = 1e-3，weight_decay = 0。（不冻结）\n    #       SGD：\n    #           Init_Epoch = 0，Freeze_Epoch = 50，UnFreeze_Epoch = 300，Freeze_Train = True，optimizer_type = 'sgd'，Init_lr = 1e-2，weight_decay = 5e-4。（冻结）\n    #           Init_Epoch = 0，UnFreeze_Epoch = 300，Freeze_Train = False，optimizer_type = 'sgd'，Init_lr = 1e-2，weight_decay = 5e-4。（不冻结）\n    #       其中：UnFreeze_Epoch可以在100-300之间调整。\n    #   （二）从0开始训练：\n    #       Init_Epoch = 0，UnFreeze_Epoch >= 300，Unfreeze_batch_size >= 16，Freeze_Train = False（不冻结训练）\n    #       其中：UnFreeze_Epoch尽量不小于300。optimizer_type = 'sgd'，Init_lr = 1e-2，mosaic = True。\n    #   （三）batch_size的设置：\n    #       在显卡能够接受的范围内，以大为好。显存不足与数据集大小无关，提示显存不足（OOM或者CUDA out of memory）请调小batch_size。\n    #       受到BatchNorm层影响，batch_size最小为2，不能为1。\n    #       正常情况下Freeze_batch_size建议为Unfreeze_batch_size的1-2倍。不建议设置的差距过大，因为关系到学习率的自动调整。\n    #----------------------------------------------------------------------------------------------------------------------------#\n    #------------------------------------------------------------------#\n    #   冻结阶段训练参数\n    #   此时模型的主干被冻结了，特征提取网络不发生改变\n    #   占用的显存较小，仅对网络进行微调\n    #   Init_Epoch          模型当前开始的训练世代，其值可以大于Freeze_Epoch，如设置：\n    #                       Init_Epoch = 60、Freeze_Epoch = 50、UnFreeze_Epoch = 100\n    #                       会跳过冻结阶段，直接从60代开始，并调整对应的学习率。\n    #                       （断点续练时使用）\n    #   Freeze_Epoch        模型冻结训练的Freeze_Epoch\n    #                       (当Freeze_Train=False时失效)\n    #   Freeze_batch_size   模型冻结训练的batch_size\n    #                       (当Freeze_Train=False时失效)\n    #------------------------------------------------------------------#\n    Init_Epoch          = 0\n    Freeze_Epoch        = 50\n    Freeze_batch_size   = 16\n    #------------------------------------------------------------------#\n    #   解冻阶段训练参数\n    #   此时模型的主干不被冻结了，特征提取网络会发生改变\n    #   占用的显存较大，网络所有的参数都会发生改变\n    #   UnFreeze_Epoch          模型总共训练的epoch\n    #                           SGD需要更长的时间收敛，因此设置较大的UnFreeze_Epoch\n    #                           Adam可以使用相对较小的UnFreeze_Epoch\n    #   Unfreeze_batch_size     模型在解冻后的batch_size\n    #------------------------------------------------------------------#\n    UnFreeze_Epoch      = 200\n    Unfreeze_batch_size = 8\n    #------------------------------------------------------------------#\n    #   Freeze_Train    是否进行冻结训练\n    #                   默认先冻结主干训练后解冻训练。\n    #------------------------------------------------------------------#\n    Freeze_Train        = True\n    \n    #------------------------------------------------------------------#\n    #   其它训练参数：学习率、优化器、学习率下降有关\n    #------------------------------------------------------------------#\n    #------------------------------------------------------------------#\n    #   Init_lr         模型的最大学习率\n    #   Min_lr          模型的最小学习率，默认为最大学习率的0.01\n    #------------------------------------------------------------------#\n    Init_lr             = 1e-2\n    Min_lr              = Init_lr * 0.01\n    #------------------------------------------------------------------#\n    #   optimizer_type  使用到的优化器种类，可选的有adam、sgd\n    #                   当使用Adam优化器时建议设置  Init_lr=1e-3\n    #                   当使用SGD优化器时建议设置   Init_lr=1e-2\n    #   momentum        优化器内部使用到的momentum参数\n    #   weight_decay    权值衰减，可防止过拟合\n    #                   adam会导致weight_decay错误，使用adam时建议设置为0。\n    #------------------------------------------------------------------#\n    optimizer_type      = \"sgd\"\n    momentum            = 0.937\n    weight_decay        = 5e-4\n    #------------------------------------------------------------------#\n    #   lr_decay_type   使用到的学习率下降方式，可选的有step、cos\n    #------------------------------------------------------------------#\n    lr_decay_type       = \"cos\"\n    #------------------------------------------------------------------#\n    #   save_period     多少个epoch保存一次权值\n    #------------------------------------------------------------------#\n    save_period         = 10\n    #------------------------------------------------------------------#\n    #   save_dir        权值与日志文件保存的文件夹\n    #------------------------------------------------------------------#\n    save_dir            = 'logs'\n    #------------------------------------------------------------------#\n    #   eval_flag       是否在训练时进行评估，评估对象为验证集\n    #                   安装pycocotools库后，评估体验更佳。\n    #   eval_period     代表多少个epoch评估一次，不建议频繁的评估\n    #                   评估需要消耗较多的时间，频繁评估会导致训练非常慢\n    #   此处获得的mAP会与get_map.py获得的会有所不同，原因有二：\n    #   （一）此处获得的mAP为验证集的mAP。\n    #   （二）此处设置评估参数较为保守，目的是加快评估速度。\n    #------------------------------------------------------------------#\n    eval_flag           = True\n    eval_period         = 10\n    #------------------------------------------------------------------#\n    #   num_workers     用于设置是否使用多线程读取数据\n    #                   开启后会加快数据读取速度，但是会占用更多内存\n    #                   内存较小的电脑可以设置为2或者0  \n    #------------------------------------------------------------------#\n    num_workers         = 4\n\n    #----------------------------------------------------#\n    #   获得图片路径和标签\n    #----------------------------------------------------#\n    train_annotation_path   = '2007_train.txt'\n    val_annotation_path     = '2007_val.txt'\n\n    #------------------------------------------------------#\n    #   设置用到的显卡\n    #------------------------------------------------------#\n    ngpus_per_node  = torch.cuda.device_count()\n    if distributed:\n        dist.init_process_group(backend=\"nccl\")\n        local_rank  = int(os.environ[\"LOCAL_RANK\"])\n        rank        = int(os.environ[\"RANK\"])\n        device      = torch.device(\"cuda\", local_rank)\n        if local_rank == 0:\n            print(f\"[{os.getpid()}] (rank = {rank}, local_rank = {local_rank}) training...\")\n            print(\"Gpu Device Count : \", ngpus_per_node)\n    else:\n        device          = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n        local_rank      = 0\n        rank            = 0\n        \n    #----------------------------------------------------#\n    #   获取classes和anchor\n    #----------------------------------------------------#\n    class_names, num_classes = get_classes(classes_path)\n\n    #------------------------------------------------------#\n    #   创建yolo模型\n    #------------------------------------------------------#\n    model = YoloBody(num_classes, phi)\n    weights_init(model)\n    if model_path != '':\n        #------------------------------------------------------#\n        #   权值文件请看README，百度网盘下载\n        #------------------------------------------------------#\n        if local_rank == 0:\n            print('Load weights {}.'.format(model_path))\n        \n        #------------------------------------------------------#\n        #   根据预训练权重的Key和模型的Key进行加载\n        #------------------------------------------------------#\n        model_dict      = model.state_dict()\n        pretrained_dict = torch.load(model_path, map_location = device)\n        load_key, no_load_key, temp_dict = [], [], {}\n        for k, v in pretrained_dict.items():\n            if k in model_dict.keys() and np.shape(model_dict[k]) == np.shape(v):\n                temp_dict[k] = v\n                load_key.append(k)\n            else:\n                no_load_key.append(k)\n        model_dict.update(temp_dict)\n        model.load_state_dict(model_dict)\n        #------------------------------------------------------#\n        #   显示没有匹配上的Key\n        #------------------------------------------------------#\n        if local_rank == 0:\n            print(\"\\nSuccessful Load Key:\", str(load_key)[:500], \"……\\nSuccessful Load Key Num:\", len(load_key))\n            print(\"\\nFail To Load Key:\", str(no_load_key)[:500], \"……\\nFail To Load Key num:\", len(no_load_key))\n            print(\"\\n\\033[1;33;44m温馨提示，head部分没有载入是正常现象，Backbone部分没有载入是错误的。\\033[0m\")\n\n    #----------------------#\n    #   获得损失函数\n    #----------------------#\n    yolo_loss    = YOLOLoss(num_classes, fp16)\n    #----------------------#\n    #   记录Loss\n    #----------------------#\n    if local_rank == 0:\n        time_str        = datetime.datetime.strftime(datetime.datetime.now(),'%Y_%m_%d_%H_%M_%S')\n        log_dir         = os.path.join(save_dir, \"loss_\" + str(time_str))\n        loss_history    = LossHistory(log_dir, model, input_shape=input_shape)\n    else:\n        loss_history    = None\n        \n    #------------------------------------------------------------------#\n    #   torch 1.2不支持amp，建议使用torch 1.7.1及以上正确使用fp16\n    #   因此torch1.2这里显示\"could not be resolve\"\n    #------------------------------------------------------------------#\n    if fp16:\n        from torch.cuda.amp import GradScaler as GradScaler\n        scaler = GradScaler()\n    else:\n        scaler = None\n\n    model_train     = model.train()\n    #----------------------------#\n    #   多卡同步Bn\n    #----------------------------#\n    if sync_bn and ngpus_per_node > 1 and distributed:\n        model_train = torch.nn.SyncBatchNorm.convert_sync_batchnorm(model_train)\n    elif sync_bn:\n        print(\"Sync_bn is not support in one gpu or not distributed.\")\n\n    if Cuda:\n        if distributed:\n            #----------------------------#\n            #   多卡平行运行\n            #----------------------------#\n            model_train = model_train.cuda(local_rank)\n            model_train = torch.nn.parallel.DistributedDataParallel(model_train, device_ids=[local_rank], find_unused_parameters=True)\n        else:\n            model_train = torch.nn.DataParallel(model)\n            cudnn.benchmark = True\n            model_train = model_train.cuda()\n\n    #----------------------------#\n    #   权值平滑\n    #----------------------------#\n    ema = ModelEMA(model_train)\n    \n    #---------------------------#\n    #   读取数据集对应的txt\n    #---------------------------#\n    with open(train_annotation_path, encoding='utf-8') as f:\n        train_lines = f.readlines()\n    with open(val_annotation_path, encoding='utf-8') as f:\n        val_lines   = f.readlines()\n    num_train   = len(train_lines)\n    num_val     = len(val_lines)\n     \n    if local_rank == 0:\n        show_config(\n            classes_path = classes_path, model_path = model_path, input_shape = input_shape, \\\n            Init_Epoch = Init_Epoch, Freeze_Epoch = Freeze_Epoch, UnFreeze_Epoch = UnFreeze_Epoch, Freeze_batch_size = Freeze_batch_size, Unfreeze_batch_size = Unfreeze_batch_size, Freeze_Train = Freeze_Train, \\\n            Init_lr = Init_lr, Min_lr = Min_lr, optimizer_type = optimizer_type, momentum = momentum, lr_decay_type = lr_decay_type, \\\n            save_period = save_period, save_dir = save_dir, num_workers = num_workers, num_train = num_train, num_val = num_val\n        )\n        #---------------------------------------------------------#\n        #   总训练世代指的是遍历全部数据的总次数\n        #   总训练步长指的是梯度下降的总次数 \n        #   每个训练世代包含若干训练步长，每个训练步长进行一次梯度下降。\n        #   此处仅建议最低训练世代，上不封顶，计算时只考虑了解冻部分\n        #----------------------------------------------------------#\n        wanted_step = 5e4 if optimizer_type == \"sgd\" else 1.5e4\n        total_step  = num_train // Unfreeze_batch_size * UnFreeze_Epoch\n        if total_step <= wanted_step:\n            if num_train // Unfreeze_batch_size == 0:\n                raise ValueError('数据集过小，无法进行训练，请扩充数据集。')\n            wanted_epoch = wanted_step // (num_train // Unfreeze_batch_size) + 1\n            print(\"\\n\\033[1;33;44m[Warning] 使用%s优化器时，建议将训练总步长设置到%d以上。\\033[0m\"%(optimizer_type, wanted_step))\n            print(\"\\033[1;33;44m[Warning] 本次运行的总训练数据量为%d，Unfreeze_batch_size为%d，共训练%d个Epoch，计算出总训练步长为%d。\\033[0m\"%(num_train, Unfreeze_batch_size, UnFreeze_Epoch, total_step))\n            print(\"\\033[1;33;44m[Warning] 由于总训练步长为%d，小于建议总步长%d，建议设置总世代为%d。\\033[0m\"%(total_step, wanted_step, wanted_epoch))\n\n    #------------------------------------------------------#\n    #   主干特征提取网络特征通用，冻结训练可以加快训练速度\n    #   也可以在训练初期防止权值被破坏。\n    #   Init_Epoch为起始世代\n    #   Freeze_Epoch为冻结训练的世代\n    #   UnFreeze_Epoch总训练世代\n    #   提示OOM或者显存不足请调小Batch_size\n    #------------------------------------------------------#\n    if True:\n        UnFreeze_flag = False\n        #------------------------------------#\n        #   冻结一定部分训练\n        #------------------------------------#\n        if Freeze_Train:\n            for param in model.backbone.parameters():\n                param.requires_grad = False\n\n        #-------------------------------------------------------------------#\n        #   如果不冻结训练的话，直接设置batch_size为Unfreeze_batch_size\n        #-------------------------------------------------------------------#\n        batch_size = Freeze_batch_size if Freeze_Train else Unfreeze_batch_size\n\n        #-------------------------------------------------------------------#\n        #   判断当前batch_size，自适应调整学习率\n        #-------------------------------------------------------------------#\n        nbs             = 64\n        lr_limit_max    = 1e-3 if optimizer_type == 'adam' else 5e-2\n        lr_limit_min    = 3e-4 if optimizer_type == 'adam' else 5e-4\n        Init_lr_fit     = min(max(batch_size / nbs * Init_lr, lr_limit_min), lr_limit_max)\n        Min_lr_fit      = min(max(batch_size / nbs * Min_lr, lr_limit_min * 1e-2), lr_limit_max * 1e-2)\n\n        #---------------------------------------#\n        #   根据optimizer_type选择优化器\n        #---------------------------------------#\n        pg0, pg1, pg2 = [], [], []  \n        for k, v in model.named_modules():\n            if hasattr(v, \"bias\") and isinstance(v.bias, nn.Parameter):\n                pg2.append(v.bias)    \n            if isinstance(v, nn.BatchNorm2d) or \"bn\" in k:\n                pg0.append(v.weight)    \n            elif hasattr(v, \"weight\") and isinstance(v.weight, nn.Parameter):\n                pg1.append(v.weight)   \n        optimizer = {\n            'adam'  : optim.Adam(pg0, Init_lr_fit, betas = (momentum, 0.999)),\n            'sgd'   : optim.SGD(pg0, Init_lr_fit, momentum = momentum, nesterov=True)\n        }[optimizer_type]\n        optimizer.add_param_group({\"params\": pg1, \"weight_decay\": weight_decay})\n        optimizer.add_param_group({\"params\": pg2})\n\n        #---------------------------------------#\n        #   获得学习率下降的公式\n        #---------------------------------------#\n        lr_scheduler_func = get_lr_scheduler(lr_decay_type, Init_lr_fit, Min_lr_fit, UnFreeze_Epoch)\n        \n        #---------------------------------------#\n        #   判断每一个世代的长度\n        #---------------------------------------#\n        epoch_step      = num_train // batch_size\n        epoch_step_val  = num_val // batch_size\n        \n        if epoch_step == 0 or epoch_step_val == 0:\n            raise ValueError(\"数据集过小，无法继续进行训练，请扩充数据集。\")\n        \n        if ema:\n            ema.updates     = epoch_step * Init_Epoch\n\n        #---------------------------------------#\n        #   构建数据集加载器。\n        #---------------------------------------#\n        train_dataset   = YoloDataset(train_lines, input_shape, num_classes, epoch_length = UnFreeze_Epoch, \\\n                                            mosaic=mosaic, mixup=mixup, mosaic_prob=mosaic_prob, mixup_prob=mixup_prob, train=True, special_aug_ratio=special_aug_ratio)\n        val_dataset     = YoloDataset(val_lines, input_shape, num_classes, epoch_length = UnFreeze_Epoch, \\\n                                            mosaic=False, mixup=False, mosaic_prob=0, mixup_prob=0, train=False, special_aug_ratio=0)\n        \n        if distributed:\n            train_sampler   = torch.utils.data.distributed.DistributedSampler(train_dataset, shuffle=True,)\n            val_sampler     = torch.utils.data.distributed.DistributedSampler(val_dataset, shuffle=False,)\n            batch_size      = batch_size // ngpus_per_node\n            shuffle         = False\n        else:\n            train_sampler   = None\n            val_sampler     = None\n            shuffle         = True\n\n        gen             = DataLoader(train_dataset, shuffle = shuffle, batch_size = batch_size, num_workers = num_workers, pin_memory=True,\n                                    drop_last=True, collate_fn=yolo_dataset_collate, sampler=train_sampler)\n        gen_val         = DataLoader(val_dataset  , shuffle = shuffle, batch_size = batch_size, num_workers = num_workers, pin_memory=True, \n                                    drop_last=True, collate_fn=yolo_dataset_collate, sampler=val_sampler)\n\n        #----------------------#\n        #   记录eval的map曲线\n        #----------------------#\n        if local_rank == 0:\n            eval_callback   = EvalCallback(model, input_shape, class_names, num_classes, val_lines, log_dir, Cuda, \\\n                                            eval_flag=eval_flag, period=eval_period)\n        else:\n            eval_callback   = None\n        \n        #---------------------------------------#\n        #   开始模型训练\n        #---------------------------------------#\n        for epoch in range(Init_Epoch, UnFreeze_Epoch):\n            #---------------------------------------#\n            #   如果模型有冻结学习部分\n            #   则解冻，并设置参数\n            #---------------------------------------#\n            if epoch >= Freeze_Epoch and not UnFreeze_flag and Freeze_Train:\n                batch_size = Unfreeze_batch_size\n                    \n                #-------------------------------------------------------------------#\n                #   判断当前batch_size，自适应调整学习率\n                #-------------------------------------------------------------------#\n                nbs             = 64\n                lr_limit_max    = 1e-3 if optimizer_type == 'adam' else 5e-2\n                lr_limit_min    = 3e-4 if optimizer_type == 'adam' else 5e-4\n                Init_lr_fit     = min(max(batch_size / nbs * Init_lr, lr_limit_min), lr_limit_max)\n                Min_lr_fit      = min(max(batch_size / nbs * Min_lr, lr_limit_min * 1e-2), lr_limit_max * 1e-2)\n                #---------------------------------------#\n                #   获得学习率下降的公式\n                #---------------------------------------#\n                lr_scheduler_func = get_lr_scheduler(lr_decay_type, Init_lr_fit, Min_lr_fit, UnFreeze_Epoch)\n                \n                for param in model.backbone.parameters():\n                    param.requires_grad = True\n\n                epoch_step      = num_train // batch_size\n                epoch_step_val  = num_val // batch_size\n\n                if epoch_step == 0 or epoch_step_val == 0:\n                    raise ValueError(\"数据集过小，无法继续进行训练，请扩充数据集。\")\n\n                if distributed:\n                    batch_size = batch_size // ngpus_per_node\n                    \n                if ema:\n                    ema.updates     = epoch_step * epoch\n                    \n                gen             = DataLoader(train_dataset, shuffle = shuffle, batch_size = batch_size, num_workers = num_workers, pin_memory=True,\n                                            drop_last=True, collate_fn=yolo_dataset_collate, sampler=train_sampler)\n                gen_val         = DataLoader(val_dataset  , shuffle = shuffle, batch_size = batch_size, num_workers = num_workers, pin_memory=True, \n                                            drop_last=True, collate_fn=yolo_dataset_collate, sampler=val_sampler)\n\n                UnFreeze_flag = True\n\n            gen.dataset.epoch_now       = epoch\n            gen_val.dataset.epoch_now   = epoch\n\n            if distributed:\n                train_sampler.set_epoch(epoch)\n\n            set_optimizer_lr(optimizer, lr_scheduler_func, epoch)\n\n            fit_one_epoch(model_train, model, ema, yolo_loss, loss_history, eval_callback, optimizer, epoch, epoch_step, epoch_step_val, gen, gen_val, UnFreeze_Epoch, Cuda, fp16, scaler, save_period, save_dir, local_rank)\n                        \n            if distributed:\n                dist.barrier()\n\n        if local_rank == 0:\n            loss_history.writer.close()","metadata":{"execution":{"iopub.status.busy":"2022-11-01T06:21:27.943111Z","iopub.execute_input":"2022-11-01T06:21:27.943404Z","iopub.status.idle":"2022-11-01T06:21:27.970143Z","shell.execute_reply.started":"2022-11-01T06:21:27.943377Z","shell.execute_reply":"2022-11-01T06:21:27.969135Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!python train.py","metadata":{"execution":{"iopub.status.busy":"2022-11-01T06:21:27.972976Z","iopub.execute_input":"2022-11-01T06:21:27.974379Z","iopub.status.idle":"2022-11-01T08:53:14.452413Z","shell.execute_reply.started":"2022-11-01T06:21:27.974335Z","shell.execute_reply":"2022-11-01T08:53:14.451195Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!ls logs","metadata":{"execution":{"iopub.status.busy":"2022-11-01T08:57:01.561074Z","iopub.execute_input":"2022-11-01T08:57:01.561516Z","iopub.status.idle":"2022-11-01T08:57:02.639575Z","shell.execute_reply.started":"2022-11-01T08:57:01.56147Z","shell.execute_reply":"2022-11-01T08:57:02.63831Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%writefile predict.py\nimport time\n\nimport cv2\nimport numpy as np\nfrom PIL import Image\n\nfrom yolo import YOLO\n\nif __name__ == \"__main__\":\n    yolo = YOLO()\n    #----------------------------------------------------------------------------------------------------------#\n    #   mode用于指定测试的模式：\n    #   'predict'           表示单张图片预测，如果想对预测过程进行修改，如保存图片，截取对象等，可以先看下方详细的注释\n    #   'video'             表示视频检测，可调用摄像头或者视频进行检测，详情查看下方注释。\n    #   'fps'               表示测试fps，使用的图片是img里面的street.jpg，详情查看下方注释。\n    #   'dir_predict'       表示遍历文件夹进行检测并保存。默认遍历img文件夹，保存img_out文件夹，详情查看下方注释。\n    #   'heatmap'           表示进行预测结果的热力图可视化，详情查看下方注释。\n    #   'export_onnx'       表示将模型导出为onnx，需要pytorch1.7.1以上。\n    #----------------------------------------------------------------------------------------------------------#\n    mode = \"predict\"\n    #-------------------------------------------------------------------------#\n    #   crop                指定了是否在单张图片预测后对目标进行截取\n    #   count               指定了是否进行目标的计数\n    #   crop、count仅在mode='predict'时有效\n    #-------------------------------------------------------------------------#\n    crop            = False\n    count           = False\n    #----------------------------------------------------------------------------------------------------------#\n    #   video_path          用于指定视频的路径，当video_path=0时表示检测摄像头\n    #                       想要检测视频，则设置如video_path = \"xxx.mp4\"即可，代表读取出根目录下的xxx.mp4文件。\n    #   video_save_path     表示视频保存的路径，当video_save_path=\"\"时表示不保存\n    #                       想要保存视频，则设置如video_save_path = \"yyy.mp4\"即可，代表保存为根目录下的yyy.mp4文件。\n    #   video_fps           用于保存的视频的fps\n    #\n    #   video_path、video_save_path和video_fps仅在mode='video'时有效\n    #   保存视频时需要ctrl+c退出或者运行到最后一帧才会完成完整的保存步骤。\n    #----------------------------------------------------------------------------------------------------------#\n    video_path      = 0\n    video_save_path = \"\"\n    video_fps       = 25.0\n    #----------------------------------------------------------------------------------------------------------#\n    #   test_interval       用于指定测量fps的时候，图片检测的次数。理论上test_interval越大，fps越准确。\n    #   fps_image_path      用于指定测试的fps图片\n    #   \n    #   test_interval和fps_image_path仅在mode='fps'有效\n    #----------------------------------------------------------------------------------------------------------#\n    test_interval   = 100\n    fps_image_path  = \"VOCdevkit/VOC2007/JPEGImages/maksssksksss0.jpg\"\n    #-------------------------------------------------------------------------#\n    #   dir_origin_path     指定了用于检测的图片的文件夹路径\n    #   dir_save_path       指定了检测完图片的保存路径\n    #   \n    #   dir_origin_path和dir_save_path仅在mode='dir_predict'时有效\n    #-------------------------------------------------------------------------#\n    dir_origin_path = \"VOCdevkit/VOC2007/JPEGImages\"\n    dir_save_path   = \"img_out/\"\n    #-------------------------------------------------------------------------#\n    #   heatmap_save_path   热力图的保存路径，默认保存在model_data下\n    #   \n    #   heatmap_save_path仅在mode='heatmap'有效\n    #-------------------------------------------------------------------------#\n    heatmap_save_path = \"model_data/heatmap_vision.png\"\n    #-------------------------------------------------------------------------#\n    #   simplify            使用Simplify onnx\n    #   onnx_save_path      指定了onnx的保存路径\n    #-------------------------------------------------------------------------#\n    simplify        = True\n    onnx_save_path  = \"model_data/models.onnx\"\n\n    if mode == \"predict\":\n        '''\n        1、如果想要进行检测完的图片的保存，利用r_image.save(\"img.jpg\")即可保存，直接在predict.py里进行修改即可。 \n        2、如果想要获得预测框的坐标，可以进入yolo.detect_image函数，在绘图部分读取top，left，bottom，right这四个值。\n        3、如果想要利用预测框截取下目标，可以进入yolo.detect_image函数，在绘图部分利用获取到的top，left，bottom，right这四个值\n        在原图上利用矩阵的方式进行截取。\n        4、如果想要在预测图上写额外的字，比如检测到的特定目标的数量，可以进入yolo.detect_image函数，在绘图部分对predicted_class进行判断，\n        比如判断if predicted_class == 'car': 即可判断当前目标是否为车，然后记录数量即可。利用draw.text即可写字。\n        '''\n        while True:\n            img = input('Input image filename:')\n            try:\n                image = Image.open(img)\n            except:\n                print('Open Error! Try again!')\n                continue\n            else:\n                r_image = yolo.detect_image(image, crop = crop, count=count)\n                r_image.show()\n\n    elif mode == \"video\":\n        capture = cv2.VideoCapture(video_path)\n        if video_save_path!=\"\":\n            fourcc  = cv2.VideoWriter_fourcc(*'XVID')\n            size    = (int(capture.get(cv2.CAP_PROP_FRAME_WIDTH)), int(capture.get(cv2.CAP_PROP_FRAME_HEIGHT)))\n            out     = cv2.VideoWriter(video_save_path, fourcc, video_fps, size)\n\n        ref, frame = capture.read()\n        if not ref:\n            raise ValueError(\"未能正确读取摄像头（视频），请注意是否正确安装摄像头（是否正确填写视频路径）。\")\n\n        fps = 0.0\n        while(True):\n            t1 = time.time()\n            # 读取某一帧\n            ref, frame = capture.read()\n            if not ref:\n                break\n            # 格式转变，BGRtoRGB\n            frame = cv2.cvtColor(frame,cv2.COLOR_BGR2RGB)\n            # 转变成Image\n            frame = Image.fromarray(np.uint8(frame))\n            # 进行检测\n            frame = np.array(yolo.detect_image(frame))\n            # RGBtoBGR满足opencv显示格式\n            frame = cv2.cvtColor(frame,cv2.COLOR_RGB2BGR)\n            \n            fps  = ( fps + (1./(time.time()-t1)) ) / 2\n            print(\"fps= %.2f\"%(fps))\n            frame = cv2.putText(frame, \"fps= %.2f\"%(fps), (0, 40), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n            \n            cv2.imshow(\"video\",frame)\n            c= cv2.waitKey(1) & 0xff \n            if video_save_path!=\"\":\n                out.write(frame)\n\n            if c==27:\n                capture.release()\n                break\n\n        print(\"Video Detection Done!\")\n        capture.release()\n        if video_save_path!=\"\":\n            print(\"Save processed video to the path :\" + video_save_path)\n            out.release()\n        cv2.destroyAllWindows()\n        \n    elif mode == \"fps\":\n        img = Image.open(fps_image_path)\n        tact_time = yolo.get_FPS(img, test_interval)\n        print(str(tact_time) + ' seconds, ' + str(1/tact_time) + 'FPS, @batch_size 1')\n\n    elif mode == \"dir_predict\":\n        import os\n\n        from tqdm import tqdm\n\n        img_names = os.listdir(dir_origin_path)\n        for img_name in tqdm(img_names):\n            if img_name.lower().endswith(('.bmp', '.dib', '.png', '.jpg', '.jpeg', '.pbm', '.pgm', '.ppm', '.tif', '.tiff')):\n                image_path  = os.path.join(dir_origin_path, img_name)\n                image       = Image.open(image_path)\n                r_image     = yolo.detect_image(image)\n                if not os.path.exists(dir_save_path):\n                    os.makedirs(dir_save_path)\n                r_image.save(os.path.join(dir_save_path, img_name.replace(\".jpg\", \".png\")), quality=95, subsampling=0)\n\n    elif mode == \"heatmap\":\n        while True:\n            img = input('Input image filename:')\n            try:\n                image = Image.open(img)\n            except:\n                print('Open Error! Try again!')\n                continue\n            else:\n                yolo.detect_heatmap(image, heatmap_save_path)\n                \n    elif mode == \"export_onnx\":\n        yolo.convert_to_onnx(simplify, onnx_save_path)\n        \n    else:\n        raise AssertionError(\"Please specify the correct mode: 'predict', 'video', 'fps', 'heatmap', 'export_onnx', 'dir_predict'.\")","metadata":{"execution":{"iopub.status.busy":"2022-11-01T08:56:04.671074Z","iopub.execute_input":"2022-11-01T08:56:04.671437Z","iopub.status.idle":"2022-11-01T08:56:04.683331Z","shell.execute_reply.started":"2022-11-01T08:56:04.671403Z","shell.execute_reply":"2022-11-01T08:56:04.682381Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%writefile yolo.py\nimport colorsys\nimport os\nimport time\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom PIL import ImageDraw, ImageFont\n\nfrom nets.yolo import YoloBody\nfrom utils.utils import (cvtColor, get_classes, preprocess_input, resize_image,\n                         show_config)\nfrom utils.utils_bbox import decode_outputs, non_max_suppression\n\n'''\n训练自己的数据集必看注释！\n'''\nclass YOLO(object):\n    _defaults = {\n        #--------------------------------------------------------------------------#\n        #   使用自己训练好的模型进行预测一定要修改model_path和classes_path！\n        #   model_path指向logs文件夹下的权值文件，classes_path指向model_data下的txt\n        #\n        #   训练好后logs文件夹下存在多个权值文件，选择验证集损失较低的即可。\n        #   验证集损失较低不代表mAP较高，仅代表该权值在验证集上泛化性能较好。\n        #   如果出现shape不匹配，同时要注意训练时的model_path和classes_path参数的修改\n        #--------------------------------------------------------------------------#\n        \"model_path\"        : 'logs/best_epoch_weights.pth',\n        \"classes_path\"      : 'model_data/voc_classes.txt',\n        #---------------------------------------------------------------------#\n        #   输入图片的大小，必须为32的倍数。\n        #---------------------------------------------------------------------#\n        \"input_shape\"       : [640, 640],\n        #---------------------------------------------------------------------#\n        #   所使用的YoloX的版本。nano、tiny、s、m、l、x\n        #---------------------------------------------------------------------#\n        \"phi\"               : 's',\n        #---------------------------------------------------------------------#\n        #   只有得分大于置信度的预测框会被保留下来\n        #---------------------------------------------------------------------#\n        \"confidence\"        : 0.5,\n        #---------------------------------------------------------------------#\n        #   非极大抑制所用到的nms_iou大小\n        #---------------------------------------------------------------------#\n        \"nms_iou\"           : 0.3,\n        #---------------------------------------------------------------------#\n        #   该变量用于控制是否使用letterbox_image对输入图像进行不失真的resize，\n        #   在多次测试后，发现关闭letterbox_image直接resize的效果更好\n        #---------------------------------------------------------------------#\n        \"letterbox_image\"   : True,\n        #-------------------------------#\n        #   是否使用Cuda\n        #   没有GPU可以设置成False\n        #-------------------------------#\n        \"cuda\"              : True,\n    }\n\n    @classmethod\n    def get_defaults(cls, n):\n        if n in cls._defaults:\n            return cls._defaults[n]\n        else:\n            return \"Unrecognized attribute name '\" + n + \"'\"\n\n    #---------------------------------------------------#\n    #   初始化YOLO\n    #---------------------------------------------------#\n    def __init__(self, **kwargs):\n        self.__dict__.update(self._defaults)\n        for name, value in kwargs.items():\n            setattr(self, name, value)\n            self._defaults[name] = value \n            \n        #---------------------------------------------------#\n        #   获得种类和先验框的数量\n        #---------------------------------------------------#\n        self.class_names, self.num_classes  = get_classes(self.classes_path)\n\n        #---------------------------------------------------#\n        #   画框设置不同的颜色\n        #---------------------------------------------------#\n        hsv_tuples = [(x / self.num_classes, 1., 1.) for x in range(self.num_classes)]\n        self.colors = list(map(lambda x: colorsys.hsv_to_rgb(*x), hsv_tuples))\n        self.colors = list(map(lambda x: (int(x[0] * 255), int(x[1] * 255), int(x[2] * 255)), self.colors))\n        self.generate()\n        \n        show_config(**self._defaults)\n\n    #---------------------------------------------------#\n    #   生成模型\n    #---------------------------------------------------#\n    def generate(self, onnx=False):\n        self.net    = YoloBody(self.num_classes, self.phi)\n        device      = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n        self.net.load_state_dict(torch.load(self.model_path, map_location=device))\n        self.net    = self.net.eval()\n        print('{} model, and classes loaded.'.format(self.model_path))\n        if not onnx:\n            if self.cuda:\n                self.net = nn.DataParallel(self.net)\n                self.net = self.net.cuda()\n\n    #---------------------------------------------------#\n    #   检测图片\n    #---------------------------------------------------#\n    def detect_image(self, image, crop = False, count = False):\n        #---------------------------------------------------#\n        #   获得输入图片的高和宽\n        #---------------------------------------------------#\n        image_shape = np.array(np.shape(image)[0:2])\n        #---------------------------------------------------------#\n        #   在这里将图像转换成RGB图像，防止灰度图在预测时报错。\n        #   代码仅仅支持RGB图像的预测，所有其它类型的图像都会转化成RGB\n        #---------------------------------------------------------#\n        image       = cvtColor(image)\n        #---------------------------------------------------------#\n        #   给图像增加灰条，实现不失真的resize\n        #   也可以直接resize进行识别\n        #---------------------------------------------------------#\n        image_data  = resize_image(image, (self.input_shape[1],self.input_shape[0]), self.letterbox_image)\n        #---------------------------------------------------------#\n        #   添加上batch_size维度\n        #---------------------------------------------------------#\n        image_data  = np.expand_dims(np.transpose(preprocess_input(np.array(image_data, dtype='float32')), (2, 0, 1)), 0)\n\n        with torch.no_grad():\n            images = torch.from_numpy(image_data)\n            if self.cuda:\n                images = images.cuda()\n            #---------------------------------------------------------#\n            #   将图像输入网络当中进行预测！\n            #---------------------------------------------------------#\n            outputs = self.net(images)\n            outputs = decode_outputs(outputs, self.input_shape)\n            #---------------------------------------------------------#\n            #   将预测框进行堆叠，然后进行非极大抑制\n            #---------------------------------------------------------#\n            results = non_max_suppression(outputs, self.num_classes, self.input_shape, \n                        image_shape, self.letterbox_image, conf_thres = self.confidence, nms_thres = self.nms_iou)\n                                                    \n            if results[0] is None: \n                return image\n\n            top_label   = np.array(results[0][:, 6], dtype = 'int32')\n            top_conf    = results[0][:, 4] * results[0][:, 5]\n            top_boxes   = results[0][:, :4]\n        #---------------------------------------------------------#\n        #   设置字体与边框厚度\n        #---------------------------------------------------------#\n        font        = ImageFont.truetype(font='model_data/simhei.ttf', size=np.floor(3e-2 * image.size[1] + 0.5).astype('int32'))\n        thickness   = int(max((image.size[0] + image.size[1]) // np.mean(self.input_shape), 1))\n        #---------------------------------------------------------#\n        #   计数\n        #---------------------------------------------------------#\n        if count:\n            print(\"top_label:\", top_label)\n            classes_nums    = np.zeros([self.num_classes])\n            for i in range(self.num_classes):\n                num = np.sum(top_label == i)\n                if num > 0:\n                    print(self.class_names[i], \" : \", num)\n                classes_nums[i] = num\n            print(\"classes_nums:\", classes_nums)\n        #---------------------------------------------------------#\n        #   是否进行目标的裁剪\n        #---------------------------------------------------------#\n        if crop:\n            for i, c in list(enumerate(top_label)):\n                top, left, bottom, right = top_boxes[i]\n                top     = max(0, np.floor(top).astype('int32'))\n                left    = max(0, np.floor(left).astype('int32'))\n                bottom  = min(image.size[1], np.floor(bottom).astype('int32'))\n                right   = min(image.size[0], np.floor(right).astype('int32'))\n                \n                dir_save_path = \"img_crop\"\n                if not os.path.exists(dir_save_path):\n                    os.makedirs(dir_save_path)\n                crop_image = image.crop([left, top, right, bottom])\n                crop_image.save(os.path.join(dir_save_path, \"crop_\" + str(i) + \".png\"), quality=95, subsampling=0)\n                print(\"save crop_\" + str(i) + \".png to \" + dir_save_path)\n\n        #---------------------------------------------------------#\n        #   图像绘制\n        #---------------------------------------------------------#\n        for i, c in list(enumerate(top_label)):\n            predicted_class = self.class_names[int(c)]\n            box             = top_boxes[i]\n            score           = top_conf[i]\n\n            top, left, bottom, right = box\n\n            top     = max(0, np.floor(top).astype('int32'))\n            left    = max(0, np.floor(left).astype('int32'))\n            bottom  = min(image.size[1], np.floor(bottom).astype('int32'))\n            right   = min(image.size[0], np.floor(right).astype('int32'))\n\n            label = '{} {:.2f}'.format(predicted_class, score)\n            draw = ImageDraw.Draw(image)\n            label_size = draw.textsize(label, font)\n            label = label.encode('utf-8')\n            print(label, top, left, bottom, right)\n            \n            if top - label_size[1] >= 0:\n                text_origin = np.array([left, top - label_size[1]])\n            else:\n                text_origin = np.array([left, top + 1])\n\n            for i in range(thickness):\n                draw.rectangle([left + i, top + i, right - i, bottom - i], outline=self.colors[c])\n            draw.rectangle([tuple(text_origin), tuple(text_origin + label_size)], fill=self.colors[c])\n            draw.text(text_origin, str(label,'UTF-8'), fill=(0, 0, 0), font=font)\n            del draw\n\n        return image\n    \n    def get_FPS(self, image, test_interval):\n        image_shape = np.array(np.shape(image)[0:2])\n        #---------------------------------------------------------#\n        #   在这里将图像转换成RGB图像，防止灰度图在预测时报错。\n        #   代码仅仅支持RGB图像的预测，所有其它类型的图像都会转化成RGB\n        #---------------------------------------------------------#\n        image       = cvtColor(image)\n        #---------------------------------------------------------#\n        #   给图像增加灰条，实现不失真的resize\n        #   也可以直接resize进行识别\n        #---------------------------------------------------------#\n        image_data  = resize_image(image, (self.input_shape[1],self.input_shape[0]), self.letterbox_image)\n        #---------------------------------------------------------#\n        #   添加上batch_size维度\n        #---------------------------------------------------------#\n        image_data  = np.expand_dims(np.transpose(preprocess_input(np.array(image_data, dtype='float32')), (2, 0, 1)), 0)\n\n        with torch.no_grad():\n            images = torch.from_numpy(image_data)\n            if self.cuda:\n                images = images.cuda()\n            #---------------------------------------------------------#\n            #   将图像输入网络当中进行预测！\n            #---------------------------------------------------------#\n            outputs = self.net(images)\n            outputs = decode_outputs(outputs, self.input_shape)\n            #---------------------------------------------------------#\n            #   将预测框进行堆叠，然后进行非极大抑制\n            #---------------------------------------------------------#\n            results = non_max_suppression(outputs, self.num_classes, self.input_shape, \n                        image_shape, self.letterbox_image, conf_thres = self.confidence, nms_thres = self.nms_iou)\n                                                  \n        t1 = time.time()\n        for _ in range(test_interval):\n            with torch.no_grad():\n                #---------------------------------------------------------#\n                #   将图像输入网络当中进行预测！\n                #---------------------------------------------------------#\n                outputs = self.net(images)\n                outputs = decode_outputs(outputs, self.input_shape)\n                #---------------------------------------------------------#\n                #   将预测框进行堆叠，然后进行非极大抑制\n                #---------------------------------------------------------#\n                results = non_max_suppression(outputs, self.num_classes, self.input_shape, \n                            image_shape, self.letterbox_image, conf_thres = self.confidence, nms_thres = self.nms_iou)\n                                \n        t2 = time.time()\n        tact_time = (t2 - t1) / test_interval\n        return tact_time\n\n    def detect_heatmap(self, image, heatmap_save_path):\n        import cv2\n        import matplotlib\n        matplotlib.use('Agg')\n        import matplotlib.pyplot as plt\n        def sigmoid(x):\n            y = 1.0 / (1.0 + np.exp(-x))\n            return y\n        #---------------------------------------------------#\n        #   获得输入图片的高和宽\n        #---------------------------------------------------#\n        image_shape = np.array(np.shape(image)[0:2])\n        #---------------------------------------------------------#\n        #   在这里将图像转换成RGB图像，防止灰度图在预测时报错。\n        #   代码仅仅支持RGB图像的预测，所有其它类型的图像都会转化成RGB\n        #---------------------------------------------------------#\n        image       = cvtColor(image)\n        #---------------------------------------------------------#\n        #   给图像增加灰条，实现不失真的resize\n        #   也可以直接resize进行识别\n        #---------------------------------------------------------#\n        image_data  = resize_image(image, (self.input_shape[1],self.input_shape[0]), self.letterbox_image)\n        #---------------------------------------------------------#\n        #   添加上batch_size维度\n        #---------------------------------------------------------#\n        image_data  = np.expand_dims(np.transpose(preprocess_input(np.array(image_data, dtype='float32')), (2, 0, 1)), 0)\n\n        with torch.no_grad():\n            images = torch.from_numpy(image_data)\n            if self.cuda:\n                images = images.cuda()\n            #---------------------------------------------------------#\n            #   将图像输入网络当中进行预测！\n            #---------------------------------------------------------#\n            outputs = self.net(images)\n            \n        outputs = [output.cpu().numpy() for output in outputs]\n        plt.imshow(image, alpha=1)\n        plt.axis('off')\n        mask    = np.zeros((image.size[1], image.size[0]))\n        for sub_output in outputs:\n            b, c, h, w = np.shape(sub_output)\n            sub_output = np.transpose(sub_output, [0, 2, 3, 1])[0]\n            score      = np.max(sigmoid(sub_output[..., 5:]), -1) * sigmoid(sub_output[..., 4])\n            score      = cv2.resize(score, (image.size[0], image.size[1]))\n            normed_score    = (score * 255).astype('uint8')\n            mask            = np.maximum(mask, normed_score)\n            \n        plt.imshow(mask, alpha=0.5, interpolation='nearest', cmap=\"jet\")\n\n        plt.axis('off')\n        plt.subplots_adjust(top=1, bottom=0, right=1,  left=0, hspace=0, wspace=0)\n        plt.margins(0, 0)\n        plt.savefig(heatmap_save_path, dpi=200)\n        print(\"Save to the \" + heatmap_save_path)\n        plt.cla()\n\n    def convert_to_onnx(self, simplify, model_path):\n        import onnx\n        self.generate(onnx=True)\n\n        im                  = torch.zeros(1, 3, *self.input_shape).to('cpu')  # image size(1, 3, 512, 512) BCHW\n        input_layer_names   = [\"images\"]\n        output_layer_names  = [\"output\"]\n        \n        # Export the model\n        print(f'Starting export with onnx {onnx.__version__}.')\n        torch.onnx.export(self.net,\n                        im,\n                        f               = model_path,\n                        verbose         = False,\n                        opset_version   = 12,\n                        training        = torch.onnx.TrainingMode.EVAL,\n                        do_constant_folding = True,\n                        input_names     = input_layer_names,\n                        output_names    = output_layer_names,\n                        dynamic_axes    = None)\n\n        # Checks\n        model_onnx = onnx.load(model_path)  # load onnx model\n        onnx.checker.check_model(model_onnx)  # check onnx model\n\n        # Simplify onnx\n        if simplify:\n            import onnxsim\n            print(f'Simplifying with onnx-simplifier {onnxsim.__version__}.')\n            model_onnx, check = onnxsim.simplify(\n                model_onnx,\n                dynamic_input_shape=False,\n                input_shapes=None)\n            assert check, 'assert check failed'\n            onnx.save(model_onnx, model_path)\n\n        print('Onnx model save as {}'.format(model_path))\n        \n    def get_map_txt(self, image_id, image, class_names, map_out_path):\n        f = open(os.path.join(map_out_path, \"detection-results/\"+image_id+\".txt\"),\"w\") \n        image_shape = np.array(np.shape(image)[0:2])\n        #---------------------------------------------------------#\n        #   在这里将图像转换成RGB图像，防止灰度图在预测时报错。\n        #   代码仅仅支持RGB图像的预测，所有其它类型的图像都会转化成RGB\n        #---------------------------------------------------------#\n        image       = cvtColor(image)\n        #---------------------------------------------------------#\n        #   给图像增加灰条，实现不失真的resize\n        #   也可以直接resize进行识别\n        #---------------------------------------------------------#\n        image_data  = resize_image(image, (self.input_shape[1],self.input_shape[0]), self.letterbox_image)\n        #---------------------------------------------------------#\n        #   添加上batch_size维度\n        #---------------------------------------------------------#\n        image_data  = np.expand_dims(np.transpose(preprocess_input(np.array(image_data, dtype='float32')), (2, 0, 1)), 0)\n\n        with torch.no_grad():\n            images = torch.from_numpy(image_data)\n            if self.cuda:\n                images = images.cuda()\n            #---------------------------------------------------------#\n            #   将图像输入网络当中进行预测！\n            #---------------------------------------------------------#\n            outputs = self.net(images)\n            outputs = decode_outputs(outputs, self.input_shape)\n            #---------------------------------------------------------#\n            #   将预测框进行堆叠，然后进行非极大抑制\n            #---------------------------------------------------------#\n            results = non_max_suppression(outputs, self.num_classes, self.input_shape, \n                        image_shape, self.letterbox_image, conf_thres = self.confidence, nms_thres = self.nms_iou)\n                                                    \n            if results[0] is None: \n                return\n\n            top_label   = np.array(results[0][:, 6], dtype = 'int32')\n            top_conf    = results[0][:, 4] * results[0][:, 5]\n            top_boxes   = results[0][:, :4]\n\n        for i, c in list(enumerate(top_label)):\n            predicted_class = self.class_names[int(c)]\n            box             = top_boxes[i]\n            score           = str(top_conf[i])\n\n            top, left, bottom, right = box\n            if predicted_class not in class_names:\n                continue\n\n            f.write(\"%s %s %s %s %s %s\\n\" % (predicted_class, score[:6], str(int(left)), str(int(top)), str(int(right)),str(int(bottom))))\n\n        f.close()\n        return ","metadata":{"execution":{"iopub.status.busy":"2022-11-01T09:03:23.763365Z","iopub.execute_input":"2022-11-01T09:03:23.763791Z","iopub.status.idle":"2022-11-01T09:03:23.783155Z","shell.execute_reply.started":"2022-11-01T09:03:23.763751Z","shell.execute_reply":"2022-11-01T09:03:23.781785Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!python predict.py","metadata":{"execution":{"iopub.status.busy":"2022-11-01T09:03:24.385652Z","iopub.execute_input":"2022-11-01T09:03:24.385989Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}